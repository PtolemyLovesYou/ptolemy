{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ptolemy","text":""},{"location":"#open-source-universal-ml-observability","title":"Open Source Universal ML Observability","text":"<p>Ptolemy is an open-source ML monitoring platform built on systems engineering principles that brings clarity to complex ML deployments. Created by engineers who value architectural rigor, performance, and flexibility, Ptolemy delivers comprehensive observability without requiring you to reinvent the wheel for each new methodology.</p> <p> Try out Ptolemy now using our Getting Started guide!</p> <p>Ptolemy is in Alpha!</p> <p>Ptolemy is still in alpha and we would love your help making it better! See our guide on Contribution to learn more about submitting feedback and contributing.</p>"},{"location":"#why-ptolemy","title":"Why Ptolemy?","text":""},{"location":"#systems-first-design","title":"Systems-First Design","text":"<p>Ptolemy organizes your ML monitoring across four logical levels (system, subsystem, component, subcomponent) and four data types (input, output, feedback, metadata). This structured yet flexible framework makes troubleshooting and optimization intuitive, even for complex ML deployments. Learn more about our data model here.</p>"},{"location":"#lightning-fast-performance","title":"Lightning-Fast Performance","text":"<p>Built with Rust and gRPC for speed-critical components, Ptolemy ingests and processes your data with minimal overhead. We've optimized every part of the stack because milliseconds matter when you're debugging production.</p>"},{"location":"#nexus-not-a-replacement","title":"Nexus, Not a Replacement","text":"<p>We don't want to replace your existing monitoring tools \u2014 they're great at what they do. Instead, Ptolemy serves as a central nexus, connecting your observability data sources through our flexible connector ecosystem and extracting insights that would otherwise remain hidden.</p>"},{"location":"#security-without-sacrifice","title":"Security Without Sacrifice","text":"<p>While we're building toward full compliance certifications, Ptolemy already includes comprehensive auditing capabilities, fine-grained access controls, and customizable permission schemes. Your sensitive ML data deserves nothing less.</p>"},{"location":"#deploy-without-drama","title":"Deploy Without Drama","text":"<p>We've built Ptolemy on battle-tested technologies you likely already have deployed. No exotic dependencies, no complex infra prerequisites \u2013\u00a0just standard components that your ops team already knows how to maintain. Deploy in minutes, not months, without fighting the endless approval battles plaguing ML tooling adoption. Because great observability shouldn't require a six-month procurement cycle.</p>"},{"location":"#built-for-extensibility","title":"Built for Extensibility","text":"<p>We understand that ML engineers are (rightfully) opinionated about their observability needs. Every ML system is unique, which is why Ptolemy prioritizes extensibility at our core. Our plugin architecture lets you integrate with your existing pipelines and tools without forcing you to adopt our opinions. Whether you're using custom metrics, proprietary data formats, or specialized visualization tools, Ptolemy's flexible APIs and connector framework adapt to your workflow \u2013 not the other way around.</p>"},{"location":"api_reference/configuration/","title":"Services Configuration","text":"<p>The API component handles all external requests and manages authentication. Its configuration is divided into two main sections: general settings and Postgres database configurations.</p>"},{"location":"api_reference/configuration/#general-configuration","title":"General Configuration","text":"<p>These settings control core API behaviors, including authentication and auditing. The sysadmin credentials are particularly important as they provide initial system access. The JWT secret is used for secure token generation and should be a strong, randomly generated value.</p> Name Description Default Value Required <code>PTOLEMY_USER</code> Sysadmin username True <code>PTOLEMY_PASS</code> Sysadmin password True <code>JWT_SECRET</code> Secret to generate JWTs. Must be base64 encoded. True <code>ENABLE_AUDITING</code> Enable auditing <code>false</code> False <code>SHUTDOWN_TIMEOUT</code> Timeout for graceful shutdown (seconds) <code>10</code> False"},{"location":"api_reference/configuration/#postgres-configuration","title":"Postgres Configuration","text":"<p>Ptolemy uses Postgres as its primary datastore. These settings define how the API connects to your Postgres instance. While default values are provided for most settings, in production environments you should explicitly configure all parameters to ensure security and reliability.</p> Name Description Default Value Required <code>POSTGRES_USER</code> Postgres username True <code>POSTGRES_PASSWORD</code> Postgres password True <code>POSTGRES_HOST</code> Postgres host <code>localhost</code> False <code>POSTGRES_PORT</code> Postgres port <code>5432</code> False <code>POSTGRES_DATABASE</code> Postgres database <code>ptolemy</code> False"},{"location":"api_reference/gql_schema/","title":"GraphQL Schema","text":"schema.gql"},{"location":"api_reference/system_diagrams/database_schema/","title":"Database Schema","text":""},{"location":"api_reference/system_diagrams/database_schema/#iam-tables","title":"IAM Tables","text":"<pre><code>erDiagram\nworkspace_user }o--|| workspace : workspace_id\nworkspace_user }o--|| users : user_id\n\nworkspace ||--o{ service_api_key : workspace_id\n\nusers ||--o{ user_api_key: user_id\n\nworkspace {\n    id uuid PK\n    name varchar\n    archived bool\n    created_at timestamp\n    updated_at timestamp\n    deleted_at timestamptz\n    deletion_reason varchar\n}\n\nusers {\n    id uuid PK\n    username varchar\n    password_hash varchar\n    display_name varchar\n    status user_status\n    is_sysadmin bool\n    is_admin bool\n    deleted_at timestamptz\n    deletion_reason varchar\n}\n\nworkspace_user {\n    id uuid PK\n    user_id uuid FK\n    workspace_id uuid FK\n    workspace_role workspace_role\n    deleted_at timestamptz\n    deletion_reason varchar\n}\n\nservice_api_key {\n    id uuid PK\n    workspace_id uuid FK\n    name varchar\n    key_hash varchar\n    key_preview varchar(16)\n    permissions api_key_permission\n    expires_at timestamptz(6)\n    deleted_at timestamptz\n    deletion_reason varchar\n}\n\nuser_api_key {\n    id uuid PK\n    user_id uuid FK\n    name varchar\n    key_hash varchar\n    key_preview varchar\n    expires_at timestamptz(6)\n    deleted_at timestamptz\n    deletion_reason varchar\n}\n</code></pre>"},{"location":"api_reference/system_diagrams/database_schema/#data-tables","title":"Data Tables","text":"<pre><code>erDiagram\n\nworkspace ||--o{ system_event : workspace_id\n\nsystem_event ||--|{ data : system_event_id\nsubsystem_event ||--|{ data : subsystem_event_id\ncomponent_event ||--|{ data: component_event_id\nsubcomponent_event ||--|{ data: subcomponent_event_id\n\ndata ||--|{ runtime : \"\"\ndata ||--o{ io : \"\"\ndata ||--o{ metadata : \"\"\n\napi_access_audit_logs ||--o| user_query : api_access_audit_log_id\nworkspace ||--o{ user_query : allowed_workspace_ids\n\nsystem_event {\n    id uuid PK\n    workspace_id uuid FK\n    name varchar\n    parameters json\n    version varchar\n    environment varchar\n}\n\nsubsystem_event {\n    id uuid PK\n    system_event_id uuid FK\n    name varchar\n    parameters json\n    version varchar\n    environment varchar\n}\n\ncomponent_event {\n    id uuid PK\n    system_event_id uuid FK\n    name varchar\n    parameters json\n    version varchar\n    environment varchar\n}\n\nsubcomponent_event {\n    id uuid PK\n    system_event_id uuid FK\n    name varchar\n    parameters json\n    version varchar\n    environment varchar\n}\n\nruntime {\n    id uuid PK\n    system_event_id uuid FK\n    subsystem_event_id uuid FK\n    component_event_id uuid FK\n    subcomponent_event_id uuid FK\n    start_time timestamp\n    end_time timestamp\n    error_type varchar\n    error_content varchar\n}\n\nio {\n    id uuid PK\n    system_event_id uuid FK\n    subsystem_event_id uuid FK\n    component_event_id uuid FK\n    subcomponent_event_id uuid FK\n    field_name varchar\n    field_value_str varchar\n    field_value_int int8\n    field_value_float float8\n    field_value_bool bool\n    field_value_json json\n    field_value_type field_value_type\n}\n\nmetadata {\n    id uuid PK\n    system_event_id uuid FK\n    subsystem_event_id uuid FK\n    component_event_id uuid FK\n    subcomponent_event_id uuid FK\n    field_name varchar\n    field_value varchar\n}\n\nuser_query {\n    id uuid PK\n    api_access_audit_log_id uuid FK\n    allowed_workspace_ids uuid[] FK\n    query_type query_type\n    access_reason access_reason\n    query_access_details varchar\n    query_text varchar\n    operation_name varchar\n    variables jsonb\n    query_metadata jsonb\n    query_start_time timestamptz not null\n    failure_details jsonb\n}\n</code></pre>"},{"location":"api_reference/system_diagrams/database_schema/#audit-tables","title":"Audit Tables","text":"<pre><code>erDiagram\n\napi_access_audit_logs ||--o{ api_auth_audit_logs : api_access_audit_log_id\napi_access_audit_logs ||--o{ iam_audit_logs : api_access_audit_log_id\napi_access_audit_logs ||--o{ record_access_audit_logs : api_access_audit_log_id\n\nservice_api_key ||--o{ api_auth_audit_logs : service_api_key_id\nuser_api_key ||--o{ api_auth_audit_logs : user_api_key_id\nusers ||--o{ api_auth_audit_logs : user_id\n\nrecord_access_audit_logs ||--|| user_query : user_query_id\nuser_query ||--|| user_query_results : user_query_id\n\napi_access_audit_logs {\n    id uuid PK\n    created_at timestamptz\n    source varchar\n    request_id uuid\n    ip_address inet\n    archive_status archive_status\n}\n\napi_auth_audit_logs {\n    id uuid PK\n    api_access_audit_log_id uuid FK\n    service_api_key_id uuid FK\n    user_api_key_id uuid FK\n    user_id uuid FK\n    auth_method auth_method\n    auth_payload_hash bytea\n    success boolean\n    failure_details jsonb\n}\n\niam_audit_logs {\n    id uuid PK\n    api_access_audit_log_id uuid FK\n    resource_id uuid\n    table_name varchar\n    operation_type operation_type\n    old_state bytea\n    new_state bytea\n    failure_reason varchar\n    query_metadata jsonb\n}\n\nuser_query_results {\n    id uuid PK\n    user_query_id uuid FK\n    failure_details jsonb\n    query_end_time timestamptz\n    query_status query_status\n    resource_usage jsonb\n}\n\nrecord_access_audit_logs {\n    id uuid PK\n    api_access_audit_log_id uuid FK\n    user_query_id uuid FK\n    operation_type operation_type\n    schema_name name\n    table_name name\n    entity_ids uuid[]\n}\n</code></pre>"},{"location":"concepts/data_model/","title":"Data Model","text":"<p>Ptolemy uses a hierarchical data model designed to capture comprehensive information about machine learning systems at various levels of granularity. This structure enables detailed tracking, debugging, and analysis of ML workflows across environments.</p>"},{"location":"concepts/data_model/#hierarchical-structure","title":"Hierarchical Structure \ud83c\udfd7\ufe0f","text":"<p>The platform organizes observability data across four tiers:</p> <ol> <li>System: The highest level, representing the entire ML application or workflow</li> <li>Subsystem: Major functional units within a system</li> <li>Component: Individual modules or services within subsystems</li> <li>Subcomponent: The smallest trackable units within components</li> </ol> <p>This hierarchical approach allows for both broad system-level insights and detailed component-level analysis.</p> Structuring Your ML System in Ptolemy <p>Tiers</p> Tier What It Represents How to Identify Example System Complete ML application Has its own API, solves a business problem Recommendation Engine Subsystem Major functional area Distinct processing phase, owned by specific team Candidate Generation Component Single-purpose unit Specific algorithm, clear inputs/outputs Vector Search Subcomponent Algorithm step Use sparingly for complex components needing detailed monitoring Query Tokenization <p>Best Practices:</p> <ul> <li>Start with Systems and Subsystems, add deeper tiers as needed</li> <li>Aim for 3-7 Subsystems per System</li> <li>Ensure data flows logically between tiers</li> <li>Use consistent granularity for similar functionality</li> </ul> <p>Common Pattern Example: <pre><code>\u251c\u2500\u2500 System: ML Service\n    \u251c\u2500\u2500 Subsystem: Data Processing\n    \u251c\u2500\u2500 Subsystem: Model Inference\n    \u2502   \u251c\u2500\u2500 Component: Algorithm A\n    \u2502   \u2514\u2500\u2500 Component: Algorithm B\n    \u2514\u2500\u2500 Subsystem: Post-Processing\n</code></pre></p>"},{"location":"concepts/data_model/#data-categories","title":"Data Categories","text":"<p>Within each tier, the platform captures six types of information:</p>"},{"location":"concepts/data_model/#1-events","title":"1. Events \u26a1","text":"<p>Events represent executions or actions at each tier. Each event includes:</p> <ul> <li>A unique identifier</li> <li>Name and version</li> <li>Parameter configurations (as JSON)</li> <li>Environment context (DEV, STAGE, PROD, etc.)</li> </ul> <p>Events form the backbone of the observability model, with each tier's events linked to its parent tier through references.</p>"},{"location":"concepts/data_model/#2-runtime-information","title":"2. Runtime Information \u23f1\ufe0f","text":"<p>Runtime captures execution details including:</p> <ul> <li>Start and end timestamps (with microsecond precision)</li> <li>Error information (type and content when applicable)</li> <li>Associated tier and event reference</li> </ul> <p>This data enables performance tracking, failure analysis, and SLA monitoring.</p>"},{"location":"concepts/data_model/#3-data-flow-inputs-outputs-and-feedback","title":"3. Data Flow: Inputs, Outputs, and Feedback \ud83d\udd04","text":"<p>Ptolemy systematically tracks the flow of data through your ML systems:</p> <p>Inputs</p> <ul> <li>Field names and typed values (supporting string, integer, float, boolean, and JSON)</li> <li>Input context and metadata</li> <li>Enables reproducibility and helps identify how varying inputs affect outcomes</li> </ul> <p>Outputs</p> <ul> <li>Results produced at each tier using the same flexible data typing system</li> <li>Captures various return formats consistently</li> </ul> <p>Feedback</p> <ul> <li>Auxiliary metrics collected during or immediately after execution</li> <li>Includes quality scores, toxicity measurements, compliance metrics, and other immediate evaluations</li> <li>Enables real-time quality assessment of model performance</li> </ul> <p>This three-part data flow tracking creates a complete picture of how information transforms throughout your ML pipeline.</p> <p>Why Four Tiers? Understanding Supersystems in Ptolemy</p> <p>The Four-Tier Architecture</p> <p>Ptolemy intentionally limits its hierarchy to four tiers (System, Subsystem, Component, Subcomponent) to balance observability with practical usability. But what about higher-level constructs?</p> <p>Supersystems: The Fifth Tier That Isn't</p> <p>Supersystems represent end-to-end workflows that span multiple systems. For example:</p> <ul> <li>A complete user conversation spanning multiple turns</li> <li>A multi-stage ML pipeline crossing service boundaries</li> <li>A business process involving several ML systems</li> </ul> <p>Rather than adding a fifth tier, Ptolemy recommends using metadata to track supersystem relationships:</p> <pre><code># Instead of:\n\u251c\u2500\u2500 Supersystem: User Conversation\n    \u2514\u2500\u2500 System: Turn Processing\n\n# Use metadata at the System level:\n\u251c\u2500\u2500 System: Turn Processing\n    \u2514\u2500\u2500 Metadata: conversation_id=abc123, turn_number=3\n</code></pre> <p>Why This Approach?</p> <ol> <li>Simplicity: Four tiers provide sufficient granularity without overwhelming complexity</li> <li>Query Flexibility: Metadata-based grouping enables more dynamic supersystem analysis</li> <li>Cross-Cutting Concerns: Some systems may participate in multiple supersystems</li> <li>Varying Lifecycles: Supersystems often have different retention and governance needs</li> </ol> <p>This approach gives you supersystem visibility while keeping it simple, stupid.</p>"},{"location":"concepts/data_model/#4-metadata","title":"4. Metadata \ud83c\udff7\ufe0f","text":"<p>Metadata provides additional context through string key-value pairs, useful for:</p> <ul> <li>Tagging executions</li> <li>Adding identifiers</li> <li>Including searchable annotations</li> <li>Linking to external systems</li> </ul>"},{"location":"concepts/data_model/#data-type-flexibility","title":"Data Type Flexibility \ud83e\udde9","text":"<p>A core principle of Ptolemy's design is the flexible handling of input, output, and feedback data. This flexibility is critical for ML observability due to the diverse nature of machine learning workloads:</p> <ol> <li> <p>Polymorphic Data Storage: Ptolemy stores values in type-specific fields (string, integer, float, boolean, or JSON) while maintaining a unified query interface.</p> </li> <li> <p>JSON Support for Complex Structures: For nested or complex data formats like prompt templates, embedding vectors, or configuration objects, the JSON type provides unlimited flexibility without requiring schema modifications.</p> </li> <li> <p>Type Safety with Runtime Flexibility: The <code>field_value_type</code> enum ensures type safety while allowing for dynamic data handling, enabling Ptolemy to adapt to various ML frameworks and model types without code changes.</p> </li> <li> <p>Single Field Conceptual Model: Although implemented as separate columns for efficiency, conceptually each field represents a single value that can be of any supported type, simplifying the developer experience.</p> </li> <li> <p>Cross-Framework Compatibility: This approach enables Ptolemy to accommodate diverse ML ecosystems, from traditional statistical models to neural networks to large language models, each with their own input/output characteristics.</p> </li> </ol> <p>This flexible type system is particularly valuable for: - LLM applications with text inputs/outputs alongside numerical configuration parameters - Multimodal models that process various data types - Ensemble systems combining different model architectures - Feature stores with heterogeneous feature types - Experimental workflows where data schemas evolve frequently</p>"},{"location":"concepts/data_model/#data-management","title":"Data Management \ud83d\uddc4\ufe0f","text":"<p>The platform implements soft deletion throughout the data model. Rather than permanently removing records, the system:</p> <ol> <li>Marks records with deletion timestamps</li> <li>Records deletion reasons</li> <li>Preserves the data for audit and analysis purposes</li> </ol> <p>This approach maintains data lineage and enables historical analysis while supporting data governance requirements.</p>"},{"location":"concepts/data_model/#schema-design-principles","title":"Schema Design Principles \ud83d\udcd0","text":"<p>The data model follows several key design principles:</p> <ol> <li>Referential Integrity: Cascading deletes ensure that related records remain consistent</li> <li>Type Safety: Enumerated types enforce data validation</li> <li>Flexible Value Storage: Different data types are accommodated through type-specific fields</li> <li>Constraint Enforcement: Check constraints ensure that records are associated with the correct tier</li> </ol>"},{"location":"concepts/data_model/#systems-engineering-alignment","title":"Systems Engineering Alignment \ud83d\udd27","text":"<p>Ptolemy's data model is deliberately structured to align with traditional systems engineering principles:</p>"},{"location":"concepts/data_model/#hierarchical-decomposition","title":"Hierarchical Decomposition","text":"<p>The four-tier structure (system, subsystem, component, subcomponent) directly mirrors the classic systems engineering approach of breaking down complex systems into manageable, functionally distinct parts. This decomposition:</p> <ol> <li>Enables Clear Boundaries: Each tier has well-defined responsibilities and interfaces</li> <li>Supports Modularity: Changes in one component can be isolated without affecting others</li> <li>Facilitates Traceability: Issues can be tracked through the hierarchy to their source</li> <li>Promotes Reusability: Well-defined components can be reused across different systems</li> </ol>"},{"location":"concepts/data_model/#separation-of-concerns","title":"Separation of Concerns","text":"<p>Ptolemy enforces good system architecture by separating different aspects of ML workflows:</p> <ol> <li>Configuration vs. Execution: Parameters are separated from runtime information</li> <li>Functional Logic vs. Performance: Events capture what happened, while runtime tracks how efficiently it occurred</li> <li>Data Flow Transparency: Explicit tracking of inputs and outputs makes data lineage clear</li> <li>Metadata Independence: Contextual information is kept separate from functional data</li> </ol>"},{"location":"concepts/data_model/#system-boundaries-and-interfaces","title":"System Boundaries and Interfaces","text":"<p>The data model explicitly captures system interfaces through:</p> <ol> <li>Defined Input/Output Contracts: Each tier's inputs and outputs are formally recorded</li> <li>Clear Parent-Child Relationships: References between tiers enforce proper hierarchical structure</li> <li>Environment Context: The environment field ensures proper separation between development, staging, and production</li> </ol>"},{"location":"concepts/data_model/#governance-and-quality-assurance","title":"Governance and Quality Assurance","text":"<p>Ptolemy's model embeds governance principles:</p> <ol> <li>Soft Deletion: Maintains audit trails and historical context</li> <li>Version Tracking: Captures evolutionary changes in systems</li> <li>Error Documentation: Explicitly tracks failure modes and error types</li> <li>Feedback Integration: Incorporates quality metrics directly into the observability framework</li> </ol>"},{"location":"concepts/data_model/#adaptability-and-evolution","title":"Adaptability and Evolution","text":"<p>The flexible type system ensures that Ptolemy can evolve alongside ML technology:</p> <ol> <li>Future-Proofing: New model types can be integrated without schema changes</li> <li>Progressive Enhancement: Systems can begin with simple metrics and add complexity over time</li> <li>Technology Independence: The data model makes no assumptions about specific ML frameworks</li> </ol> <p>By adhering to these systems engineering principles, Ptolemy not only provides observability but also gently guides organizations toward better ML system architecture. The very act of instrumenting ML systems with Ptolemy encourages developers to think systematically about system boundaries, interfaces, and component responsibilities - leading to more maintainable, debuggable, and robust ML applications.</p> <p>To learn more about Ptolemy's data model, check out our System Diagrams.</p>"},{"location":"concepts/iam/","title":"\ud83d\udd10 Identity &amp; Access Management","text":"<p>Ptolemy provides a comprehensive access management system that helps teams collaborate effectively while maintaining strong security controls. Our role-based approach makes it easy to manage who can access what, whether they're querying data through our interface or connecting through API integrations. Let's dive into how it all works.</p>"},{"location":"concepts/iam/#workspaces","title":"\ud83c\udfe2 Workspaces","text":"<p>Ptolemy organizes observability data into workspaces, providing a flexible way to manage access and permissions. Think of workspaces as separate environments where you can group related data and team members. Users can belong to multiple workspaces, and each workspace can have as many users as needed.</p> <p>\ud83c\udf10 Decentralized Design</p> <p>We've designed Ptolemy's workspace management with clear boundaries and distributed control in mind. Here's how it works: When creating a workspace, you must specify its initial admin - this can be yourself or another user. From there, workspace management is fully independent. Even system-level admins can create workspaces but can't delete them - only workspace admins have that power. This ensures each workspace maintains its autonomy and data governance. Most teams operate perfectly well with a single workspace. Multiple workspaces are primarily useful when you need strict isolation between different sets of observability data - for example, separating development and production environments, or maintaining boundaries between different business units. For some organizations, multiple Ptolemy deployments might make more sense than multiple workspaces. Consider this approach if you have distinct VPCs, different compliance requirements across teams, or need to optimize for latency across geographic regions.</p>"},{"location":"concepts/iam/#roles-and-permissions","title":"\ud83d\udc65 Roles and Permissions","text":""},{"location":"concepts/iam/#workspace-roles","title":"Workspace Roles","text":"<p>Each workspace member is assigned one of these roles:</p>"},{"location":"concepts/iam/#user","title":"\ud83d\udc64 User","text":"<ul> <li>View and query workspace data</li> <li>Access workspace details and service API keys</li> <li>Perfect for team members who need to analyze data but don't need administrative access</li> </ul>"},{"location":"concepts/iam/#manager","title":"\ud83d\udcca Manager","text":"<ul> <li>Everything a User can do</li> <li>Create and delete service API keys</li> <li>Ideal for team leads who need to manage integrations and data access</li> </ul>"},{"location":"concepts/iam/#admin","title":"\u2699\ufe0f Admin","text":"<ul> <li>Everything a Manager can do</li> <li>Add or remove workspace members</li> <li>Assign and modify user roles</li> <li>Delete the workspace</li> <li>Best for project owners and those responsible for workspace governance</li> </ul>"},{"location":"concepts/iam/#system-level-roles","title":"System-level Roles","text":""},{"location":"concepts/iam/#admin_1","title":"\ud83d\udee1\ufe0f Admin","text":"<ul> <li>Create and manage workspaces</li> <li>Add and remove users from Ptolemy</li> <li>Oversee system-wide access and permissions</li> </ul>"},{"location":"concepts/iam/#sysadmin","title":"\ud83d\udd27 Sysadmin","text":"<ul> <li>A special role designed with privacy in mind</li> <li>Limited to user management (create/delete)</li> <li>Intentionally restricted from accessing workspace data</li> <li>Helps sysadmins avoid unnecessary exposure to PII/PHI</li> <li>Only one sysadmin account per system, configured at the system level</li> </ul>"},{"location":"concepts/iam/#api-keys","title":"\ud83d\udd11 API Keys","text":"<p>Ptolemy offers two types of API keys to fit different authentication needs:</p>"},{"location":"concepts/iam/#user-api-keys","title":"\ud83d\udc64 User API Keys","text":"<ul> <li>Tied to individual user accounts</li> <li>Provide read-only access to data in the user's workspaces</li> <li>Inherit all IAM permissions from the user</li> <li>Intended for integrations, management, and data exploration</li> </ul>"},{"location":"concepts/iam/#service-api-keys","title":"\ud83e\udd16 Service API Keys","text":"<ul> <li>Owned by workspaces</li> <li>Can be created by workspace managers or admins</li> <li>Support flexible permission levels:<ul> <li>\ud83d\udcca Read-only: For data consumption and analysis</li> <li>\ud83d\udce5 Write-only: For data ingestion</li> <li>\ud83d\udd04 Read-write: For full data access</li> </ul> </li> <li>Intended for deployments, data integrations, and automations</li> </ul> <p>Both types of API keys can be configured with optional expiration dates \u23f1\ufe0f, which can be made mandatory at the system level. For detailed configuration options, check out our guide on Configuration.</p> <p>This role-based access control system ensures that your data remains secure while staying accessible to those who need it. Each permission level is carefully designed to balance security with usability, making it easy to manage access as your team and data needs grow. \ud83d\ude80</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Ptolemy is an open-source project and we encourage you to contribute, no matter your skill level or background! Please read our Code of Conduct and materials on development before contributing. Happy coding!  </p>"},{"location":"contributing/#core-maintainers","title":"Core Maintainers","text":""},{"location":"contributing/#raz-besaleli","title":"Raz Besaleli","text":"<p>ML/Engineering |  Tbilisi/NYC</p> <p> </p>"},{"location":"contributing/#anton-calmis","title":"Anton Calm\u00ee\u0219","text":"<p>Design |  Tbilisi</p> <p></p>"},{"location":"contributing/#contributors","title":"Contributors","text":""},{"location":"contributing/code_of_conduct/","title":"Code of Conduct","text":"<p>Ptolemy's mission is to make building robust, safe AI easier for everyone. As an ML tooling project, we have a responsibility to actively combat bias and promote inclusion in AI development. We recognize that understanding the diversity of human experience is critical to our development as a scientific field \u2013 our goal is to build not just monitoring tools, but a community where everyone can contribute their unique perspectives to make AI better and more equitable.</p> <p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers are committed to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"contributing/code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>\ud83c\udf1f Using welcoming and inclusive language</li> <li>\ud83c\udfad Respecting differing viewpoints and experiences</li> <li>\ud83e\udd14 Gracefully accepting constructive criticism</li> <li>\ud83d\udca1 Focusing on what is best for the community</li> <li>\ud83d\udcaa Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>\u274c The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>\u274c Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>\u274c Public or private harassment</li> <li>\u274c Publishing others' private information, such as physical or electronic addresses, without explicit permission</li> <li>\u274c Misgendering or deadnaming</li> <li>\u274c Other conduct which could reasonably be considered inappropriate in a professional setting</li> <li>\u274c Dismissing or attacking inclusion-oriented requests</li> </ul>"},{"location":"contributing/code_of_conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"contributing/code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project email address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"contributing/development_process/","title":"Development Process","text":""},{"location":"contributing/development_process/#branching-strategy","title":"\ud83c\udf33 Branching Strategy","text":"<p>We use a trunk-based development model where all changes are integrated into the <code>main</code> branch. Instead of maintaining multiple long-lived branches, we use tags and releases to manage different versions of the software.</p>"},{"location":"contributing/development_process/#versioning-strategy","title":"\ud83c\udff7\ufe0f Versioning Strategy","text":"<p>We follow a modified semantic versioning scheme that includes development, beta, and release versions. Here's how our versioning works:</p> <p>\ud83d\ude91 On Hotfixes</p> <p>When a critical bug needs to be fixed in a released version (say, 1.2.0) while main already contains work toward the next minor version (1.3.0-alpha.N), we use a temporary branch from the release tag. We create a branch from the v1.2.0 tag, make the necessary fix, and tag it as 1.2.1-beta.1 for testing. After verification, we release it as 1.2.1 and merge the hotfix back into main. This approach lets us make urgent fixes without interfering with ongoing development work, while still maintaining our versioning scheme and eventually getting all changes back into main. While this introduces a temporary branch, it's very short-lived (usually hours or days) and only used for critical fixes that need to avoid picking up in-progress work from main.</p>"},{"location":"contributing/development_process/#development-versions","title":"\ud83d\udee0\ufe0f Development Versions","text":"<ul> <li>Format: <code>X.Y.Z-alpha.N+{git_hash}</code></li> <li>Example: <code>1.2.3-alpha.1+a1b2c3d</code></li> <li>These are automatically generated for each commit to <code>main</code></li> <li>The git hash helps track exactly which commit produced the build</li> </ul>"},{"location":"contributing/development_process/#beta-versions","title":"\ud83e\uddea Beta Versions","text":"<ul> <li>Format: <code>X.Y.Z-beta.N</code></li> <li>Example: <code>1.2.3-beta.1</code></li> <li>Beta numbers match their corresponding dev versions</li> <li>For example, <code>1.2.3-alpha.3+a1b2c3d</code> would become <code>1.2.3-beta.3</code></li> </ul>"},{"location":"contributing/development_process/#release-candidate-versions-major-releases-only","title":"\ud83c\udfaf Release Candidate Versions (Major Releases Only)","text":"<ul> <li>Format: <code>X.Y.Z-rc.N</code></li> <li>Example: <code>2.0.0-rc.1</code></li> <li>Used only before major version changes</li> <li>Released after beta phase for additional testing</li> </ul>"},{"location":"contributing/development_process/#release-versions","title":"\u2728 Release Versions","text":"<ul> <li>Format: <code>X.Y.Z</code></li> <li>Example: <code>1.2.3</code></li> <li>Released after successful beta/RC phase</li> <li>Drops all prerelease identifiers</li> <li>Next development version bumps the patch: <code>1.2.4-alpha.1+{git_hash}</code></li> </ul> <p>\ud83d\udcdd Version Flow Example</p> <pre><code>1.2.3-alpha.1+a1b2c3d  # Initial development version\n1.2.3-alpha.2+e4f5g6h  # More development\n1.2.3-alpha.3+i7j8k9l  # Ready for first beta\n1.2.3-beta.3         # First beta release\n1.2.3-alpha.4+m0n1o2p  # Fix based on beta feedback\n1.2.3-alpha.5+q3r4s5t  # More fixes\n1.2.3-alpha.6+u6v7w8x  # Ready for second beta\n1.2.3-beta.6         # Second beta release\n1.2.3                # Final release\n1.3.0-alpha.1+y9z0a1b  # Start next minor version\n</code></pre>"},{"location":"contributing/development_process/#development-workflow","title":"\ud83d\udc77 Development Workflow","text":"<ol> <li>Always branch from latest <code>main</code></li> <li>Make your changes in small, focused commits</li> <li>Write or update tests as needed</li> <li>Run the test suite locally</li> <li>Push your changes to your fork</li> <li>Open a Pull Request against <code>main</code></li> <li>Once approved, your changes will be merged to <code>main</code></li> <li>CI will automatically create a dev version with your changes</li> </ol>"},{"location":"contributing/development_process/#release-process","title":"\ud83d\ude80 Release Process","text":"<ol> <li>\ud83d\udcbb Development Phase</li> <li>All work happens on <code>main</code></li> <li>Each merge triggers a dev release (<code>X.Y.Z-alpha.N+{git_hash}</code>)</li> <li> <p>Changes are tested in development builds</p> </li> <li> <p>\ud83e\uddea Beta Phase</p> </li> <li>When ready for wider testing, a beta is tagged</li> <li>Beta version matches latest dev number</li> <li> <p>Example: <code>1.2.3-alpha.5+abc123</code> \u2192 <code>1.2.3-beta.5</code></p> </li> <li> <p>\ud83c\udfaf Release Candidate Phase (Major Versions Only)</p> </li> <li>After successful beta phase</li> <li>Used for additional testing of major changes</li> <li> <p>Example: <code>2.0.0-beta.5</code> \u2192 <code>2.0.0-rc.1</code></p> </li> <li> <p>\u2728 Release Phase</p> </li> <li>After successful testing</li> <li>Drops prerelease identifiers</li> <li>Example: <code>1.2.3-beta.5</code> \u2192 <code>1.2.3</code></li> <li>Next dev version bumps major, minor, or patch number depending on the changes planned for the next release</li> <li>Example patch: <code>1.2.3</code> \u2192 <code>1.2.4-alpha.1+{git_hash}</code></li> <li>Example minor: <code>1.2.3</code> \u2192 <code>1.3.0-alpha.1+{git_hash}</code></li> <li>Example major: <code>1.2.3</code> \u2192 <code>2.0.0-alpha.1+{git_hash}</code></li> </ol>"},{"location":"contributing/development_process/#package-publishing","title":"\ud83d\udce6 Package Publishing","text":"<p>Only the following versions are published to package registries: - Beta releases (<code>X.Y.Z-beta.N</code>) - Release candidates (<code>X.Y.Z-rc.N</code>) - Final releases (<code>X.Y.Z</code>)</p> <p>Development versions (<code>X.Y.Z-alpha.N+{git_hash}</code>) are built but not published.</p> <p>\ud83d\udcc8 Major, Minor, and Patch Releases</p> <p>Patch releases (1.2.3 \u2192 1.2.4) should be reserved for bug fixes, performance improvements, and security updates that don't change the public API. While we typically run these through a quick beta cycle for testing, they should be relatively straightforward changes that can be tested and released quickly. This is also how we handle hotfixes \u2013 through an accelerated beta cycle that might last just a day or two for urgent issues.</p> <p>Minor version bumps (1.2.0 \u2192 1.3.0) are our main planning unit and represent meaningful feature additions. Each minor version should have a clear set of planned features or improvements, and work on these happens through development versions (1.3.0-alpha.N) until the features are ready for beta testing. This gives us a natural way to group related changes, plan our roadmap, and communicate upcoming features to users. It also means we can continue to release patches to the current minor version (1.2.4, 1.2.5) while working on the next one (1.3.0).</p>"},{"location":"contributing/development_process/#testing-and-stability","title":"\ud83c\udfaf Testing and Stability","text":"<p>While we use a trunk-based approach with a single <code>main</code> branch, we maintain stability through our pre-release process:</p>"},{"location":"contributing/development_process/#development-builds","title":"\ud83d\udee0\ufe0f Development Builds","text":"<ul> <li>Every commit to <code>main</code> produces a dev build</li> <li>These builds undergo automated testing</li> <li>Developers can use these builds for early testing</li> <li>Unstable but fast feedback loop</li> </ul>"},{"location":"contributing/development_process/#beta-testing","title":"\ud83e\uddea Beta Testing","text":"<ul> <li>Beta releases mark code that's ready for wider testing</li> <li>Used to catch issues before final release</li> <li>Allows external users to test new features</li> <li>Important for catching real-world usage issues</li> <li>Multiple beta releases (beta.1, beta.2, etc.) may be needed</li> </ul>"},{"location":"contributing/development_process/#release-candidates","title":"\ud83c\udfaf Release Candidates","text":"<ul> <li>Extra stability gate for major versions</li> <li>Full regression testing</li> <li>Used to ensure backward compatibility</li> <li>Final chance for breaking issue discovery</li> </ul> <p>This staged approach lets us maintain the simplicity of trunk-based development while ensuring proper testing and stability. Each stage (dev \u2192 beta \u2192 rc \u2192 release) represents increasing levels of stability and testing confidence.</p>"},{"location":"contributing/issues_commits_prs/","title":"Issue, Pull Request &amp; Commit Guidelines","text":""},{"location":"contributing/issues_commits_prs/#issues","title":"\ud83c\udfab Issues","text":"<p>Every code change should start with an issue. Issues help track work, discuss implementation details, and maintain a clear project history.</p>"},{"location":"contributing/issues_commits_prs/#one-pr-per-issue-policy","title":"\ud83d\udd04 One PR Per Issue Policy","text":"<p>We generally follow a one-PR-per-issue policy, with some pragmatic exceptions:</p>"},{"location":"contributing/issues_commits_prs/#why-one-pr-per-issue","title":"\u2728 Why One PR Per Issue?","text":"<ul> <li>\ud83d\udd0d Clear traceability between changes and their motivations</li> <li>\ud83d\udce6 Forces proper task breakdown and manageable PR sizes</li> <li>\ud83d\udc40 Makes code reviews more focused and effective</li> <li>\u23ea Enables clean feature rollbacks if needed</li> <li>\ud83d\udcc8 Provides clear project progress tracking</li> </ul>"},{"location":"contributing/issues_commits_prs/#allowed-exceptions","title":"\ud83c\udfad Allowed Exceptions","text":"<ul> <li>\ud83d\udd28 Trivial changes (like typo fixes) may not need an issue</li> <li>\ud83d\udd17 Multiple small, tightly related issues might be addressed in one PR (with clear documentation)</li> <li>\ud83d\udd04 If implementation reveals an issue should split into multiple PRs, or multiple issues should combine, pause and restructure</li> </ul> <p>When deviating from one-PR-per-issue, document your reasoning in the PR description.</p>"},{"location":"contributing/issues_commits_prs/#issue-template","title":"\ud83d\udcdd Issue Template","text":"<pre><code># Problem\n&lt;!-- What needs to be done? --&gt;\n\n# Proposed Solution\n&lt;!-- How do you plan to solve it? --&gt;\n\n# Additional Context\n&lt;!-- Any extra information that might help? --&gt;\n\n# Acceptance Criteria\n&lt;!-- What needs to be true for this to be complete? --&gt;\n</code></pre>"},{"location":"contributing/issues_commits_prs/#conventional-commits","title":"\ud83d\udc8c Conventional Commits","text":"<p>We use conventional commits to maintain clear and standardized commit messages. Each commit message should follow this format:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n</code></pre>"},{"location":"contributing/issues_commits_prs/#types","title":"\ud83c\udff7\ufe0f Types","text":"<ul> <li><code>feat</code>: \u2728 New feature</li> <li><code>fix</code>: \ud83d\udc1b Bug fix</li> <li><code>docs</code>: \ud83d\udcda Documentation changes</li> <li><code>style</code>: \ud83d\udc85 Code style changes (formatting, missing semi-colons, etc)</li> <li><code>refactor</code>: \u267b\ufe0f Code refactoring</li> <li><code>perf</code>: \u26a1\ufe0f Performance improvements</li> <li><code>test</code>: \ud83e\uddea Adding missing tests</li> <li><code>chore</code>: \ud83d\udd27 Build process or auxiliary tool changes</li> </ul>"},{"location":"contributing/issues_commits_prs/#examples","title":"\ud83d\udccb Examples","text":"<pre><code>feat(api): add endpoint for model metrics\nfix(worker): resolve memory leak in batch processing\ndocs(readme): update installation instructions\nperf(client): optimize large dataset handling\n</code></pre>"},{"location":"contributing/issues_commits_prs/#pull-request-template","title":"\ud83d\ude80 Pull Request Template","text":"<p>When opening a pull request, please use the following template:</p> <pre><code># Description\n&lt;!-- What does this PR do? --&gt;\n\n# Related Issue\n&lt;!-- Link to the issue this PR addresses --&gt;\nCloses #[issue-number]\n\n# Type of Change\n&lt;!-- delete options that are not relevant --&gt;\n- \ud83d\ude80 New feature\n- \ud83d\udd27 Bug fix\n- \ud83d\udcda Documentation\n- \ud83d\udd28 Breaking change\n- \u26a1\ufe0f Performance improvement\n- \ud83e\uddea Test updates\n\n# Testing\n&lt;!-- How were these changes tested? --&gt;\n\n# Breaking Changes\n&lt;!-- Does this PR introduce breaking changes? If yes, describe the impact and migration steps --&gt;\n\n# Checklist\n- [ ] My code follows conventional commit guidelines\n- [ ] I have added tests that prove my fix/feature works\n- [ ] New and existing tests pass locally\n- [ ] I have updated relevant documentation\n- [ ] I have added metrics/monitoring for new features (if applicable)\n</code></pre>"},{"location":"contributing/issues_commits_prs/#additional-pr-guidelines","title":"\ud83c\udf1f Additional PR Guidelines","text":"<ul> <li>\ud83c\udfaf Keep PRs focused and reasonably sized</li> <li>\ud83d\udd17 Link to related issues or discussions</li> <li>\ud83d\udcac Respond to review comments promptly</li> <li>\ud83d\udd04 Update your PR with main when there are conflicts</li> <li>\ud83d\udcf8 Add screenshots or code examples for UI or API changes</li> </ul>"},{"location":"getting_started/","title":"Overview","text":"<p>Ptolemy is platform-agnostic and can be deployed in a number of different ways. If you want to learn more about Ptolemy's features, check out our Quickstart Guide to get Ptolemy running on Docker Compose.</p> <p>For getting Ptolemy's development environment set up, go to our guide on Contributing.</p>"},{"location":"getting_started/building_documentation/","title":"Building Documentation","text":"<p>Ptolemy uses MkDocs to build documentation. Below are instructions to generate and run Ptolemy's documentation.</p> <p>Before starting, you must have <code>uv</code> installed. If you're using a Mac, you can install <code>uv</code> with <code>brew</code>: <pre><code>brew install uv\n</code></pre></p> <p>If you are using another operating system or don't want to use <code>brew</code>, you can find instructions on how to install <code>uv</code> in their documentation.</p> <p>To generate Ptolemy's docs, clone the <code>ptolemy</code> GitHub repo:</p> <pre><code>git clone https://github.com/PtolemyLovesYou/ptolemy.git\n</code></pre> <p>Then, start the mkdocs server: <pre><code>make docs\n</code></pre></p> <p>You should be able to find the docs at <code>http://localhost:8080</code> in your web browser.</p>"},{"location":"getting_started/installation_docker_compose/","title":"Install with Docker Compose","text":"<p>Ptolemy is an open-source, free tool with several different deployment options. Below is a quickstart guide on how to deploy Ptolemy on your local machine with Docker.</p>"},{"location":"getting_started/installation_docker_compose/#deploy-with-docker","title":"Deploy with Docker","text":"<p>To deploy Ptolemy with Docker, you must have Docker installed on your computer. See here for instructions on how to install Docker.</p> <p>First, make a directory and navigate to it: <pre><code>mkdir ptolemy-quickstart &amp;&amp; cd ptolemy-quickstart\n</code></pre></p> <p>Then, download the <code>docker-compose.yml</code> file: <pre><code>wget -O docker-compose.yml https://raw.githubusercontent.com/PtolemyLovesYou/ptolemy/main/docker-compose.quickstart.yml\n</code></pre></p> <p>Run <code>docker compose</code> to start the containers: <pre><code>docker compose up -d # omit the -d flag to keep the docker compose logs in your terminal\n</code></pre></p> <p>To verify that everything is up and running, navigate to <code>http://localhost:8501</code> in your web browser and verify that the UI loads.</p>"},{"location":"getting_started/usage/","title":"Usage","text":"<p>Prerequisites</p> <p>Ptolemy partitions observability data into workspace. To create a workspace, you will need to create an admin user in the UI, then create a workspace.</p> <p>To write data to workspaces, you can create a Service API Key in the workspace settings. To learn more about Ptolemy's access management system, read our guide on Identity &amp; Access Management </p> <p>Here's a simple example of Ptolemy in action!</p>"},{"location":"getting_started/usage/#client-python","title":"Client (Python)","text":""}]}